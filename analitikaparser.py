import datetime
import time
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.common.by import By
import undetected_chromedriver as uc
from urllib3.exceptions import ReadTimeoutError
import traceback
import psycopg2
import requests
from bs4 import BeautifulSoup

from OpenRouterApi import summorize_text, maintheme_text

conn = psycopg2.connect(dbname="ItmosnikIko", host="127.0.0.1", user="Alex", password="alex")
cursor = conn.cursor()

year = datetime.date.today().year
month_to_number = {'—è–Ω–≤–∞—Ä—è': "01", '—Ñ–µ–≤—Ä–∞–ª—è': "02", '–º–∞—Ä—Ç–∞': "03", '–∞–ø—Ä–µ–ª—è': "04", '–º–∞—è': "05", '–∏—é–Ω—è': "06",
                   '–∏—é–ª—è': "07", '–∞–≤–≥—É—Å—Ç–∞': "08", '—Å–µ–Ω—Ç—è–±—Ä—è': "09", '–æ–∫—Ç—è–±—Ä—è': "10", '–Ω–æ—è–±—Ä—è': "11", '–¥–µ–∫–∞–±—Ä—è': "12"}

def get_links(links, titles, imgs):
    try:
        url = "https://nsp.ru/analitika"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
        }

        response = requests.get(url, headers=headers)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")
        cards = soup.find_all("app-material-card", class_="small")
        # –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î

        for el in cards:
            try:
                header = el.find("div", class_="h5")
                a = el.find('a', href=True)
                href = a['href']
                if href:
                    new_link = "https://nsp.ru/" + href
                else:
                    new_link = ""

                """# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤ –ë–î
                cursor.execute("SELECT 1 FROM news WHERE origin_link = %s", (new_link,))
                if cursor.fetchone():
                    continue"""

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–∫–∏
                links.append(new_link)
                titles.append(header.get_text(strip=True))

                img_tag = el.find("div", class_='card-img')
                source_tag = img_tag.find("source", attrs={"srcset": True})

                if source_tag:
                    img_url = source_tag["srcset"]
                else:
                    img_url = ""
                    print("‚ùå srcset –Ω–µ –Ω–∞–π–¥–µ–Ω")
                imgs.append(img_url)


                #print(f"üÜï –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è: {new_link}")
            except Exception as e:
                pass

        cursor.close()
        conn.close()

    except Exception as e:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:", e)

    return links, titles, imgs


def chek_news(link, times):
    options = uc.ChromeOptions()
    # options.headless = True
    # options.add_argument("--no-sandbox")
    options.add_argument("--disable-blink-features=AutomationControlled")

    driver = uc.Chrome(options=options)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    }
    text = ""

    try:
        driver.get(link)
        time.sleep(2)

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏
        wait = WebDriverWait(driver, 10)
        date_div = wait.until(EC.presence_of_element_located(
            (By.CLASS_NAME, "topline-info-date"))
        )

        # –ü–æ–ª—É—á–∞–µ–º –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä: "14 –º–∞—è –≤ 16:46")
        date_text = date_div.text.strip()
        month = month_to_number[date_text.split()[1]]
        day = date_text.split()[0]
        date = f"{year}-{month}-{day}"
        times.append(date)



        response = requests.get(link, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        content = soup.find_all('p')
        for el in content:
            if el.get_text(strip=True):
                text += el.get_text(strip=True) + " "

        if text:
            text = summorize_text(text)


    except ReadTimeoutError:
        print("‚è± –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ:", link)
    except Exception:
        traceback.print_exc()
        print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏:", link)

    finally:
        driver.quit()

    return text, times


links = []
titles = []
times = []
imgs = []
texts = []
count = 10
new_links, titles, imgs = get_links(links, titles, imgs)
zamena = ["–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:", "–û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã —Å—Ç–∞—Ç—å–∏:", "–ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞:", "–ì–ª–∞–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã:", " –°—É—Ç—å —Å—Ç–∞—Ç—å–∏:",
          "–û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã:", "–û—Å–Ω–æ–≤–Ω—ã–µ –ø—É–Ω–∫—Ç—ã:", "–ì–ª–∞–≤–Ω—ã–µ —Ç–µ–∑–∏—Å—ã —Å—Ç–∞—Ç—å–∏:",
          "–í–æ—Ç –∫—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–∑–∏—Å–æ–≤ —Å—Ç–∞—Ç—å–∏:", "–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–∑–∏—Å—ã —Å—Ç–∞—Ç—å–∏:", "–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–∑–∏—Å—ã:"]

for link in range(10):
    text, times = chek_news(new_links[link], times)
    new_text = text
    for old in zamena:
        new_text = new_text.replace(old, "")

    themetext = maintheme_text(new_text)
    print(themetext.title())

    texts.append(new_text)


print("Len Links: ", len(links))
print("Len titles: ", len(titles))
print("Len times: ", len(times))
print("Len imgs: ", len(imgs))
print("Len texts: ", len(texts))
check_title = []
for i in range(len(texts)):
    check_title.append(titles[i])
    parametrs = (titles[i], times[i], texts[i], imgs[i], new_links[i])
    print(f"Link: {new_links[i]}\nTitle: {titles[i]}\nTime: {times[i]}\nImg Link: {imgs[i]}\n Text: {texts[i]}\n\n")
    """cursor.execute('INSERT INTO news (title, date, text, image_link, origin_link) '
                   'VALUES (%s, %s, %s, %s, %s)', parametrs)
    conn.commit()"""
