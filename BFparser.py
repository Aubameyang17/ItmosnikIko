import re
from urllib3.exceptions import ReadTimeoutError
import traceback
import psycopg2
import requests
from bs4 import BeautifulSoup

from OpenRouterApi import summorize_text

conn = psycopg2.connect(dbname="ItmosnikIko", host="127.0.0.1", user="Alex", password="alex")
cursor = conn.cursor()

def get_links(links, titles, times, imgs):
    try:
        url = "https://www.fontanka.ru/realty/"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
        }

        response = requests.get(url, headers=headers)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")
        cards = soup.find_all("div", class_=re.compile("wrap_"))

        # –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î

        for el in cards:
            try:
                header = el.find("a", class_=re.compile("header_"))
                new_link = header.get("href")

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤ –ë–î
                cursor.execute("SELECT 1 FROM news WHERE origin_link = %s", (new_link,))
                if cursor.fetchone():
                    continue

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–∫–∏
                links.append(new_link)
                titles.append(header.get_text(strip=True))

                time_element = el.find("div", class_=re.compile("cell_"))
                times.append(time_element.get_text(strip=True) if time_element else "")

                img_tag = el.find("img")
                img_url = img_tag.get("src") if img_tag else ""
                imgs.append(img_url)

                #print(f"üÜï –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è: {new_link}")
            except Exception as e:
                pass

        cursor.close()
        conn.close()

    except Exception as e:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:", e)

    return links, titles, times, imgs


def chek_news(link):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    }
    text = ""

    try:
        response = requests.get(link, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        if 'longreads' in link:
            # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–æ–º "tn-atom"
            content = soup.find_all(class_="tn-atom")
            for el in content:
                if el.get_text(strip=True):
                    text += el.get_text(strip=True) + " "
        else:
            # –û–±—ã—á–Ω–∞—è —Å—Ç–∞—Ç—å—è
            article = soup.find(class_=re.compile("articleContent_"))
            if article:
                text = article.get_text(strip=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ summarizer (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if text:
            text = summorize_text(text)

    except ReadTimeoutError:
        print("‚è± –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ:", link)
    except Exception:
        traceback.print_exc()
        print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏:", link)

    return text


links = []
titles = []
times = []
imgs = []
texts = []
count = 10
new_links, titles, times, imgs = get_links(links, titles, times, imgs)


for link in range(10):
    text = chek_news(new_links[link])
    texts.append(text)


print("Len Links: ", len(links))
print("Len titles: ", len(titles))
print("Len times: ", len(times))
print("Len imgs: ", len(imgs))
print("Len texts: ", len(texts))
check_title = []
for i in range(len(texts)):
    check_title.append(titles[i])
    parametrs = (titles[i], times[i], texts[i], imgs[i], new_links[i])
    print(f"Link: {new_links[i]}\nTitle: {titles[i]}\nTime: {times[i]}\nImg Link{imgs[i]}\n Text: {texts[i]}\n\n")